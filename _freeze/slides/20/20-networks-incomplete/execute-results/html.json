{
  "hash": "51e600ae621b4545048bb41e7c4587d6",
  "result": {
    "markdown": "---\ntitle: Networks\nsubtitle: Lecture 20\ntitle-slide-attributes:\n  data-background-image: ../vizdata-bg.png\n  data-background-size: 800px, cover\n  data-slide-number: none\nformat: revealjs\nhighlight-style: a11y\nexecute:\n  code-link: true\n  warning: true\neditor_options: \n  chunk_output_type: console\n---\n\n\n# Warm up\n\n## Setup {.smaller}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# load packages\nlibrary(tidyverse)\nlibrary(tidytext)\nlibrary(ggtext)\nlibrary(glue)\nlibrary(ggwordcloud)\nlibrary(ggraph)\nlibrary(igraph)\n\n# set theme for ggplot2\nggplot2::theme_set(ggplot2::theme_minimal(base_size = 14))\n\n# set width of code output\noptions(width = 65)\n\n# set figure parameters for knitr\nknitr::opts_chunk$set(\n  fig.width = 7, # 7\" width\n  fig.asp = 0.618, # the golden ratio\n  fig.retina = 3, # dpi multiplier for displaying HTML output on retina\n  fig.align = \"center\", # center align figures\n  dpi = 300 # higher dpi, sharper image\n)\n```\n:::\n\n\n# Text data\n\n##  {.smaller}\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n::: task\nDo you recognize the following text?\n:::\n\n<br>\n\n> Whenever I get gloomy with the state of the world, I think about the arrivals gate at Heathrow airport. General opinion started to make out that we live in a world of hatred and greed, but I don't see that. Seems to me that love is everywhere. Often it's not particularly dignified or newsworthy but it's always there. Fathers and sons, mothers and daughters, husbands and wives, boyfriends, girlfriends, old friends. When the planes hit the Twin Towers, as far as I know, none of the phone calls from people on board were messages of hate or revenge, they were all messages of love. If you look for it, I've got a sneaky feeling, you'll find that love actually is all around.\n\n## Text as data\n\nText can be represented as data in a variery of ways:\n\n-   String: Character vector\n\n-   Corpus: Raw strings annotated with additional metadata and details\n\n-   Document-term matrix: Sparse matrix describing a collection (i.e., a corpus) of documents with one row for each document and one column for each term, with word counts (or another measure of how common the word is in that text) as values\n\n## Tidy text {.smaller}\n\n-   Each row is a **token**\n    -   A token can be a word, bigram (two words), ngram (n words), sentence, paragraph, etc.\n-   Each column is a variable\n-   Each type of observational unit is a table\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlove_actually |>\n  slice_head(n = 6)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 4\n  scene  line speaker dialogue                                   \n  <dbl> <dbl> <chr>   <chr>                                      \n1     1     1 (Man)   'Whenever I get gloomy with the state of t…\n2     2     2 Billy   ♪ I feel it in my fingers ♪ I feel it in m…\n3     2     3 Joe     I'm afraid you did it again, Bill.         \n4     2     4 Billy   It's just I know the old version so well, …\n5     2     5 Joe     Well, we all do. That's why we're making t…\n6     2     6 Billy   Right, OK, let's go. ♪ I feel it in my fin…\n```\n:::\n:::\n\n\n## Tokenize into words {.smaller}\n\nWith `tidytext::unnest_tokens()`:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlove_actually |>\n  unnest_tokens(\n    output = word,    # first argument is output\n    input = dialogue, # second argument is input\n    token = \"words\"   # third argument is token, with default \"words\"\n    )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 9,899 × 4\n   scene  line speaker word    \n   <dbl> <dbl> <chr>   <chr>   \n 1     1     1 (Man)   whenever\n 2     1     1 (Man)   i       \n 3     1     1 (Man)   get     \n 4     1     1 (Man)   gloomy  \n 5     1     1 (Man)   with    \n 6     1     1 (Man)   the     \n 7     1     1 (Man)   state   \n 8     1     1 (Man)   of      \n 9     1     1 (Man)   the     \n10     1     1 (Man)   world   \n# ℹ 9,889 more rows\n```\n:::\n:::\n\n\n## Most common words {.smaller}\n\n::: task\nWhy do these words appear so commonly in Love Actually?\n:::\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlove_actually |>\n  unnest_tokens(word, dialogue) |>\n  count(word, sort = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1,770 × 2\n   word      n\n   <chr> <int>\n 1 you     334\n 2 i       300\n 3 the     263\n 4 a       201\n 5 and     199\n 6 to      199\n 7 it      150\n 8 is      124\n 9 of      112\n10 no      111\n# ℹ 1,760 more rows\n```\n:::\n:::\n\n\n## Stop words {.smaller}\n\n-   In computing, stop words are words which are filtered out before or after processing of natural language data (text)\n\n-   They usually refer to Most common words in a language, but there is not a single list of stop words used by all natural language processing tools\n\n::: columns\n::: {.column width=\"50%\"}\n**English**\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nget_stopwords(language = \"[Ee]nglish\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 175 × 2\n   word      lexicon \n   <chr>     <chr>   \n 1 i         snowball\n 2 me        snowball\n 3 my        snowball\n 4 myself    snowball\n 5 we        snowball\n 6 our       snowball\n 7 ours      snowball\n 8 ourselves snowball\n 9 you       snowball\n10 your      snowball\n# ℹ 165 more rows\n```\n:::\n:::\n\n:::\n\n::: {.column width=\"50%\"}\n**Spanish**\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nget_stopwords(language = \"[Ss]panish\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 308 × 2\n   word  lexicon \n   <chr> <chr>   \n 1 de    snowball\n 2 la    snowball\n 3 que   snowball\n 4 el    snowball\n 5 en    snowball\n 6 y     snowball\n 7 a     snowball\n 8 los   snowball\n 9 del   snowball\n10 se    snowball\n# ℹ 298 more rows\n```\n:::\n:::\n\n:::\n:::\n\n## Most common words\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlove_actually %>%\n  unnest_tokens(word, dialogue) %>%\n  anti_join(stop_words) %>%\n  count(word, sort = TRUE)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nJoining with `by = join_by(word)`\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1,355 × 2\n   word           n\n   <chr>      <int>\n 1 christmas     49\n 2 yeah          48\n 3 er            45\n 4 love          40\n 5 erm           39\n 6 sir           28\n 7 portuguese    25\n 8 god           23\n 9 bye           21\n10 time          20\n# ℹ 1,345 more rows\n```\n:::\n:::\n\n\n## Portuguese?!\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlove_actually %>%\n  filter(str_detect(dialogue, \"[Pp]ortuguese\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 25 × 4\n   scene  line speaker dialogue                                  \n   <dbl> <dbl> <chr>   <chr>                                     \n 1    32   360 woman   Unfortunately, she cannot speak French, j…\n 2    33   367 Jamie   (Pidgin Portuguese) Bello. Er, bella. Er,…\n 3    38   421 Aurelia (Portuguese) Thank you very much but no. …\n 4    38   423 Aurelia (Portuguese)Just don't go eating it all y…\n 5    39   426 Aurelia (Portuguese) Nao! Eu peco imensa desculpa…\n 6    39   430 Aurelia (Portuguese) Fuck - it's cold!            \n 7    39   432 Aurelia (Portuguese) This stuff better be good.   \n 8    39   434 Aurelia (Portuguese) I don't want to drown saving…\n 9    39   436 Aurelia (Portuguese) What kind of an idiot doesn'…\n10    39   438 Aurelia (Portuguese) Try not to disturb the eels. \n# ℹ 15 more rows\n```\n:::\n:::\n\n\n## Data cleaning\n\n-   Remove language identifiers\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlove_actually <- love_actually %>%\n  mutate(dialogue = str_remove(dialogue, \"(Portuguese)\"))\n```\n:::\n\n\n-   Take another look\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlove_actually %>%\n  filter(str_detect(dialogue, \"[Pp]ortuguese\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 0 × 4\n# ℹ 4 variables: scene <dbl>, line <dbl>, speaker <chr>,\n#   dialogue <chr>\n```\n:::\n:::\n\n\n## Most common words\n\nwithout \"Portuguese\"\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlove_actually %>%\n  unnest_tokens(word, dialogue) %>%\n  anti_join(stop_words) %>%\n  count(word, sort = TRUE)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nJoining with `by = join_by(word)`\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1,354 × 2\n   word          n\n   <chr>     <int>\n 1 christmas    49\n 2 yeah         48\n 3 er           45\n 4 love         40\n 5 erm          39\n 6 sir          28\n 7 god          23\n 8 bye          21\n 9 time         20\n10 ah           19\n# ℹ 1,344 more rows\n```\n:::\n:::\n\n\n## Visualizing Most common words (top 10) {.smaller}\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](20-networks-incomplete_files/figure-revealjs/unnamed-chunk-13-1.png){fig-align='center' width=2100}\n:::\n:::\n\n\n\n## Visualizing Most common words (freq) {.smaller}\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](20-networks-incomplete_files/figure-revealjs/unnamed-chunk-14-1.png){fig-align='center' width=2100}\n:::\n:::\n\n\n\n## Visualizing Most common words (color) {.smaller}\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](20-networks-incomplete_files/figure-revealjs/unnamed-chunk-15-1.png){fig-align='center' width=2100}\n:::\n:::\n\n\n## Visualizing the most common words\n\nUse `ggtext::element_textbox_simple()` to add color to title\n\n\n",
    "supporting": [
      "20-networks-incomplete_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}